{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dropout, LeakyReLU, BatchNormalization, Input, Concatenate, Activation, concatenate\nfrom keras.initializers import RandomNormal\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.utils import plot_model\nimport numpy as np\nimport cv2\nimport PIL\nfrom PIL import Image\nimport random\nimport h5py\nimport os\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport cv2\nimport keras\nfrom keras.preprocessing import image\nfrom keras.engine import Layer\nfrom keras.layers import Conv2D, Conv3D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate\nfrom keras.layers import Activation, Dense, Dropout, Flatten\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import TensorBoard\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\nfrom skimage.transform import resize\nfrom skimage.io import imsave\nfrom time import time\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nfrom PIL import Image, ImageFile\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"def create_model(image_shape):\n    # Prepare the kernel initializer values\n    weight_init = RandomNormal(stddev=0.02)\n    # Prepare the Input layer\n    net_input = Input((image_shape))\n    # Download mobile net, and use it as the base.\n    mobile_net_base = MobileNetV2(\n        include_top=False,\n        input_shape=image_shape,\n        weights='imagenet'\n    )\n    mobilenet = mobile_net_base(net_input)\n    \n    # Encoder block #\n    # 224x224\n    conv1 = Conv2D(32, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(net_input)\n    conv1 = LeakyReLU(alpha=0.2)(conv1)\n    \n    # 112x112\n    conv2 = Conv2D(64, (3, 3), strides=(1, 1), padding='same', kernel_initializer=weight_init)(conv1)\n    conv2 = LeakyReLU(alpha=0.2)(conv2)\n\n    # 112x112\n    conv3 = Conv2D(64, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv2)\n    conv3 =  Activation('relu')(conv3)\n\n    # 56x56\n    conv4 = Conv2D(256, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv3)\n    conv4 = Activation('relu')(conv4)\n\n    # 28x28\n    conv4_ = Conv2D(256, (3, 3), strides=(1, 1), padding='same', kernel_initializer=weight_init)(conv4)\n    conv4_ = Activation('relu')(conv4_)\n\n    # 28x28\n    conv5 = Conv2D(512, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv4_)\n    conv5 = Activation('relu')(conv5)\n\n    # 14x14\n    conv5_ = Conv2D(256, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv5)\n    conv5_ = Activation('relu')(conv5_)\n    \n    #7x7\n    # Fusion layer - Connects MobileNet with our encoder\n    conc = concatenate([mobilenet, conv5_])\n    fusion = Conv2D(512, (1, 1), padding='same', kernel_initializer=weight_init)(conc)\n    fusion = Activation('relu')(fusion)\n    \n    # Skip fusion layer\n    skip_fusion = concatenate([fusion, conv5_])\n    \n    # Decoder block #\n    # 7x7\n    decoder = Conv2DTranspose(512, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(skip_fusion)\n    decoder = Activation('relu')(decoder)\n    decoder = Dropout(0.25)(decoder)\n\n    # Skip layer from conv5 (with added dropout)\n    skip_4_drop = Dropout(0.25)(conv5)\n    skip_4 = concatenate([decoder, skip_4_drop])\n    \n    # 14x14\n    decoder = Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(skip_4)\n    decoder = Activation('relu')(decoder)\n    decoder = Dropout(0.25)(decoder)\n\n    # Skip layer from conv4_ (with added dropout)\n    skip_3_drop = Dropout(0.25)(conv4_)\n    skip_3 = concatenate([decoder, skip_3_drop])\n    \n    # 28x28\n    decoder = Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(skip_3)\n    decoder = Activation('relu')(decoder)\n    decoder = Dropout(0.25)(decoder)\n\n    # 56x56\n    decoder = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(decoder)\n    decoder = Activation('relu')(decoder)\n    decoder = Dropout(0.25)(decoder)\n\n    # 112x112\n    decoder = Conv2DTranspose(64, (3, 3), strides=(1, 1), padding='same', kernel_initializer=weight_init)(decoder)\n    decoder = Activation('relu')(decoder)\n\n    # 112x112\n    decoder = Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(decoder)\n    decoder = Activation('relu')(decoder)\n    \n    # 224x224\n    # Ooutput layer, with 2 channels (a and b)\n    output_layer = Conv2D(2, (1, 1), activation='tanh')(decoder)\n\n    model = Model(net_input, output_layer)\n    model.compile(Adam(lr=0.0002), loss='mse', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Diagram\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def graph_training_data(epochs, training_data, validation_data, y_label, title):\n\n    fig = go.Figure()\n\n    fig.add_trace(\n        go.Scatter(\n            x=np.arange(1, epochs+1), mode='lines+markers', y=training_data,\n            marker=dict(color=\"mediumpurple\"), name=\"Training\"))\n\n    fig.add_trace(\n        go.Scatter(\n            x=np.arange(1, epochs+1), mode='lines+markers', y=validation_data,\n            marker=dict(color=\"forestgreen\"), name=\"Validation\"))\n\n    fig.update_layout(title_text=title, yaxis_title=y_label,\n                      xaxis_title=\"Epochs\", template=\"plotly_white\")\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Get prediction from the model based of the 'L' grayscale image\ndef get_pred(model, image_l):\n    # Repeat the L value to match input shape\n    image_l_R = np.repeat(image_l[..., np.newaxis], 3, -1)\n    image_l_R = image_l_R.reshape((1, 224, 224, 3))\n    # Normalize the input\n    image_l_R = (image_l_R.astype('float32') - 127.5) / 127.5\n    # Make prediction\n    prediction = model.predict(image_l_R)\n    # Normalize the output\n    pred = (prediction[0].astype('float32') * 127.5) + 127.5\n    \n    return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine an 'L' grayscale image with an 'AB' image, and convert to RGB for display or use\ndef get_LAB(image_l, image_ab):\n    image_l = image_l.reshape((224, 224, 1))\n    image_lab = np.concatenate((image_l, image_ab), axis=2)\n    image_lab = image_lab.astype(\"uint8\")\n    image_rgb = cv2.cvtColor(image_lab, cv2.COLOR_LAB2RGB)\n    image_rgb = Image.fromarray(image_rgb)\n    return image_rgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create some samples of black and white images combined, to show input/output\ndef create_sample(model, images_gray, amount):\n    path = \"/kaggle/working/\"\n    samples = []\n    for i in range(amount):\n        # Select random images\n        r = random.randint(0, images_gray.shape[0])\n        # Get the model's prediction\n        pred = get_pred(model, images_gray[r])\n        # Combine input and output to LAB image\n        image = get_LAB(images_gray[r], pred)\n        # Get number of images in output folder\n        count = len(os.listdir(path))\n        # Create new combined image and save it\n        new_image = Image.new('RGB', (448, 224))\n        gray_image = Image.fromarray(images_gray[r])\n        new_image.paste(gray_image, (0,0))\n        new_image.paste(image, (224, 0))\n        # Saving the image with the current count of images (to make it unique)\n        # and the index of the image, so that it can be found if needed\n        new_image.save(path + str(count)+('_%i.png' % r))\n        samples.append(new_image)\n    return samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, gray, ab, epochs, batch_size):\n    # Setup the training input data (grayscale images)\n    train_in = gray\n    # Convert the shape from (224, 224, 1) to (224, 224, 3) by copying the value to match MobileNet's requirements\n    train_in = np.repeat(train_in[..., np.newaxis], 3, -1)\n    \n    train_out = ab\n    # Normalize the data\n    train_in = (train_in.astype('float32') - 127.5) / 127.5\n    train_out = (train_out.astype('float32') - 127.5) / 127.5\n\n    history = model.fit(\n        train_in,\n        train_out,\n        epochs=epochs,\n        validation_split=0.05,\n        batch_size=batch_size\n    )\n    \n    return history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images_gray = np.load(\"../input/image-colorization/l/gray_scale.npy\")\nimages_ab = np.load(\"../input/image-colorization/ab/ab/ab1.npy\")\n\n# Set batch size and epochs for training run\nBATCH_SIZE = 15\nEPOCHS = 10\n\n# Create the model through the function above\nmodel = create_model((224, 224, 3))\n\n# Train the model and keep history for graphing\nhistory = train(model, images_gray[:3000], images_ab[:3000], EPOCHS, BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Graphing the training data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph_training_data(EPOCHS, history.history['loss'], history.history['val_loss'], 'Loss', \"Loss while training\")\ngraph_training_data(EPOCHS, history.history['accuracy'], history.history['val_accuracy'], 'Accuracy', \"Accuracy while training\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = \"../input/babyes/innocence-1432027.jpg\"\n#test = os.listdir(test_path)\nfor imgName in test:\n    color_me = []\n    img = img_to_array(load_img(test))\n    img = resize(img ,(224,224))\n    color_me.append(img)\n    color_me = np.array(color_me, dtype=float)\n    color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]\n    #color_me = color_me.reshape(color_me.shape+(1,))\n    color_me = np.repeat(color_me[..., np.newaxis], 3,-1)\n    \n    output = model.predict(color_me)\n    output = output * 128\n# Output colorizations\nfor i in range(len(output)):\n    result = np.zeros((224, 224, 3))\n    result[:,:,0] = color_me[i][:,:,0]\n    result[:,:,1:] = output[i]\n    imsave(\"result1.png\", lab2rgb(result))\n    x=lab2rgb(result)\n    plt.imshow(x)\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}